"""
è®°å¿† / å›å¿†ç±»ï¼ˆä»é•¿æ—¶è®°å¿†ä¸­æå–ç›¸å…³çš„çŸ¥è¯†ï¼‰

è¦ç´ è¯†åˆ«
ä¸Šä¸‹æ–‡å›å¿†



"""

from typing_extensions import Annotated
from langchain_core.tools import tool
from langgraph.prebuilt import InjectedState
from pydantic import BaseModel, Field
from Config.LLM_Client import llm, lock


class RecognizingInput(BaseModel):
    query: str = Field(description="ä»»åŠ¡æŒ‡ä»¤")


@tool(
    "è¦ç´ è¯†åˆ«",
    description="å½“ä»»åŠ¡æŒ‡ä»¤ä¸­å­˜åœ¨å½±å“ä»»åŠ¡å®Œæˆçš„å…³é”®ä¿¡æ¯è¦ç´ ï¼ˆå¦‚æ—¶é—´ã€ä¸»ä½“ã€è§„åˆ™ã€çº¦æŸæ¡ä»¶ï¼‰æ—¶ï¼Œéœ€è¦é€šè¿‡è¯†åˆ«å·¥å…·ä»æŒ‡ä»¤ä¸­æå–å‡ºè¿™äº›æ ¸å¿ƒè¦ç´ ä¿¡æ¯ã€‚",
    args_schema=RecognizingInput,
)
def recognizing(query: str) -> str:
    """
    ä»»åŠ¡æŒ‡ä»¤ä¸­å­˜åœ¨å½±å“ä»»åŠ¡å®Œæˆçš„å…³é”®ä¿¡æ¯è¦ç´ ï¼ˆå¦‚æ—¶é—´ã€ä¸»ä½“ã€è§„åˆ™ã€çº¦æŸæ¡ä»¶ï¼‰æ—¶ï¼Œéœ€è¦é€šè¿‡è¯†åˆ«å·¥å…·ä»æŒ‡ä»¤ä¸­æå–å‡ºè¿™äº›æ ¸å¿ƒè¦ç´ ä¿¡æ¯ã€‚
    """
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†è¦ç´ è¯†åˆ«æ¨¡å—ï¼Œä½ çš„ä»»åŠ¡æ˜¯ä»ä»»åŠ¡æŒ‡ä»¤ä¸­æå–å‡ºä»»åŠ¡çš„å…³é”®çŸ¥è¯†è¦ç´ ã€‚
    ä»»åŠ¡æŒ‡ä»¤æ˜¯ï¼š{query}ã€‚
    è¾“å‡ºæ ¼å¼ä¸ºï¼š[è¦ç´ 1, è¦ç´ 2, ...]
    """
    with lock:
        # æµå¼è¾“å‡º
        chunks  = []
        print(f"\nğŸ’¡è®¤çŸ¥å·¥å…·=è¦ç´ è¯†åˆ«")
        for chunk in llm.stream(prompt):
            print(chunk.content, end="", flush=True)
            chunks.append(chunk.content)

        response = "".join(chunks)

        # # å®Œæ•´è¾“å‡º
        # response = llm.invoke(prompt).content
        # print(f"\nğŸ’¡è®¤çŸ¥å·¥å…·=è¦ç´ è¯†åˆ«\n{response}")

    return response


class RecallingInput(BaseModel):
    query: str = Field(description="ä»»åŠ¡æŒ‡ä»¤")
    messages: Annotated[list, InjectedState("messages")] = Field(description="å†å²ä¸Šä¸‹æ–‡")


@tool(
    "ä¸Šä¸‹æ–‡å›å¿†",
    description="å½“ä»»åŠ¡éœ€è¦è°ƒç”¨é•¿æ—¶è®°å¿†æ¥æ¨è¿›æ—¶ï¼Œéœ€è¦é€šè¿‡å›å¿†å·¥å…·ä»å†å²ä¸Šä¸‹æ–‡è°ƒå–ä¸å½“å‰ä»»åŠ¡ç›¸å…³çš„å†…å®¹ã€‚",
    args_schema=RecallingInput,
)
def recalling(query: str, messages: Annotated[list, InjectedState("messages")]) -> str:
    """
    å½“ä»»åŠ¡éœ€è¦è°ƒç”¨é•¿æ—¶è®°å¿†æ¥æ¨è¿›æ—¶ï¼Œéœ€è¦é€šè¿‡å›å¿†å·¥å…·ä»å†å²ä¸Šä¸‹æ–‡è°ƒå–ä¸å½“å‰ä»»åŠ¡ç›¸å…³çš„å†…å®¹ã€‚
    """
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸Šä¸‹æ–‡å›å¿†æ¨¡å—ï¼Œä½ çš„ä»»åŠ¡æ˜¯ä»å†å²ä¸Šä¸‹æ–‡ä¸­ï¼Œè°ƒå–ä¸å½“å‰ä»»åŠ¡ç›¸å…³çš„å†…å®¹ã€‚
    ä»»åŠ¡æŒ‡ä»¤æ˜¯ï¼š{query}ã€‚
    ä¸Šä¸‹æ–‡æ˜¯ï¼š{messages}
    è¾“å‡ºæ ¼å¼ä¸ºï¼šä¿¡æ¯1, ä¿¡æ¯2, ...
    """
    with lock:
        # æµå¼è¾“å‡º
        chunks  = []
        print(f"\nğŸ’¡è®¤çŸ¥å·¥å…·=ä¸Šä¸‹æ–‡å›å¿†")
        for chunk in llm.stream(prompt):
            print(chunk.content, end="", flush=True)
            chunks.append(chunk.content)

        response = "".join(chunks)

        # # å®Œæ•´è¾“å‡º
        # response = llm.invoke(prompt).content
        # print(f"\nğŸ’¡è®¤çŸ¥å·¥å…·=ä¸Šä¸‹æ–‡å›å¿†\n{response}")

    return response
