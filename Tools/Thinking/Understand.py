"""
ç†è§£ç±»ï¼ˆä»å£å¤´ã€ä¹¦é¢å’Œå›¾åƒç­‰äº¤æµå½¢å¼çš„æ•™å­¦ä¿¡æ¯ä¸­å»ºæ„æ„ä¹‰ï¼‰

ä»»åŠ¡è§£é‡Š
ä»»åŠ¡ä¸¾ä¾‹
"""

from langchain_core.tools import tool
from langgraph.prebuilt import InjectedState
from pydantic import BaseModel, Field
from Config.LLM_Client import llm, lock


class InterpretingInput(BaseModel):
    query: str = Field(description="ä»»åŠ¡æŒ‡ä»¤")


@tool(
    "ä»»åŠ¡è§£é‡Š",
    description="å¦‚æœä»»åŠ¡æŒ‡ä»¤æ¯”è¾ƒæŠ½è±¡ï¼Œè§£é‡Šæˆå…·ä½“çš„ä»»åŠ¡å†…å®¹",
    args_schema=InterpretingInput,
)
def interpreting(query: str) -> str:
    """
    ç†è§£ä»»åŠ¡æŒ‡ä»¤ï¼Œå¹¶ç»™å‡ºè‡ªå·±çš„ç†è§£ã€‚
    """
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡è§£é‡Šæ¨¡å—ï¼Œä½ çš„ä»»åŠ¡æ˜¯æŠŠç›¸å¯¹æŠ½è±¡çš„ä»»åŠ¡æŒ‡ä»¤ï¼Œè§£é‡Šæˆå…·ä½“çš„ä»»åŠ¡å†…å®¹ã€‚
    ä»»åŠ¡æŒ‡ä»¤æ˜¯ï¼š{query}ã€‚
    è¾“å‡ºæ ¼å¼ä¸ºï¼šè§£é‡Šå†…å®¹
    """
    with lock:
        # æµå¼è¾“å‡º
        chunks  = []
        print(f"\nğŸ’¡è®¤çŸ¥å·¥å…·=ä»»åŠ¡è§£é‡Š")
        for chunk in llm.stream(prompt):
            print(chunk.content, end="", flush=True)
            chunks.append(chunk.content)

        response = "".join(chunks)

        # # å®Œæ•´è¾“å‡º
        # response = llm.invoke(prompt).content
        # print(f"\nğŸ’¡è®¤çŸ¥å·¥å…·=ä»»åŠ¡è§£é‡Š\n{response}")

    return response


class ExemplifyingInput(BaseModel):
    query: str = Field(description="ä»»åŠ¡æŒ‡ä»¤")


@tool(
    "ä»»åŠ¡ä¸¾ä¾‹",
    description="å¦‚æœå½“å‰ä»»åŠ¡æŒ‡ä»¤å†…åŒ…å«æŠ½è±¡çš„æ¦‚å¿µï¼Œéœ€è¦ç»™å‡ºç›¸ä¼¼çš„ä»»åŠ¡æ ·ä¾‹æ¥å¸®åŠ©ç†è§£ã€‚",
    args_schema=ExemplifyingInput,
)
def exemplifying(query: str) -> str:
    """
    é’ˆå¯¹ä»»åŠ¡æŒ‡ä»¤ï¼Œç»™å‡ºç›¸ä¼¼çš„ä»»åŠ¡æ ·ä¾‹ã€‚
    """
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡ä¸¾ä¾‹æ¨¡å—ï¼Œä½ çš„ä»»åŠ¡æ˜¯é’ˆå¯¹ä»»åŠ¡æŒ‡ä»¤å†…åŒ…å«çš„æŠ½è±¡æ¦‚å¿µï¼Œç»™å‡ºç›¸ä¼¼çš„ä»»åŠ¡æ ·ä¾‹ã€‚
    ä»»åŠ¡æŒ‡ä»¤æ˜¯ï¼š{query}ã€‚
    è¾“å‡ºæ ¼å¼ä¸ºï¼šæ ·ä¾‹1, æ ·ä¾‹2, ...
    """
    with lock:
        # æµå¼è¾“å‡º
        chunks  = []
        print(f"\nğŸ’¡è®¤çŸ¥å·¥å…·=ä»»åŠ¡ä¸¾ä¾‹")
        for chunk in llm.stream(prompt):
            print(chunk.content, end="", flush=True)
            chunks.append(chunk.content)

        response = "".join(chunks)

        # # å®Œæ•´è¾“å‡º
        # response = llm.invoke(prompt).content
        # print(f"\nğŸ’¡è®¤çŸ¥å·¥å…·=ä»»åŠ¡ä¸¾ä¾‹\n{response}")

    return response
