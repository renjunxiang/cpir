"""
åº”ç”¨ç±»ï¼ˆåœ¨ç»™å®šçš„æƒ…æ™¯ä¸­æ‰§è¡Œæˆ–ä½¿ç”¨ç¨‹åºï¼‰

ç›´æ¥ä½œç­”
"""

from langchain_core.tools import tool
from langgraph.prebuilt import InjectedState
from pydantic import BaseModel, Field
from Config.LLM_Client import llm, lock


class AnswerInput(BaseModel):
    """é—®é¢˜çš„è¾“å…¥"""

    query: str = Field(description="ç”¨æˆ·æŒ‡ä»¤")


@tool(
    "ç›´æ¥ä½œç­”",
    description="å½“ä»»åŠ¡çš„ä¸Šä¸‹æ–‡ä¿¡æ¯è¶³å¤Ÿæ—¶ï¼Œåˆ©ç”¨å¤§æ¨¡å‹æœ¬èº«çš„èƒ½åŠ›è¿›è¡Œä½œç­”ï¼Œå¹¶ç»“æŸä»»åŠ¡ã€‚",
    args_schema=AnswerInput,
)
def direct_answer(query: str) -> str:
    """
    å½“ä»»åŠ¡çš„ä¸Šä¸‹æ–‡ä¿¡æ¯è¶³å¤Ÿæ—¶ï¼Œåˆ©ç”¨å¤§æ¨¡å‹æœ¬èº«çš„èƒ½åŠ›è¿›è¡Œä½œç­”ï¼Œå¹¶ç»“æŸä»»åŠ¡ã€‚
    """
    prompt = f"""
    æ ¹æ®ä»»åŠ¡æŒ‡ä»¤ï¼Œç›´æ¥å›ç­”é—®é¢˜ã€‚
    ä»»åŠ¡æŒ‡ä»¤æ˜¯ï¼š{query}ã€‚
    è¾“å‡ºä»»åŠ¡ç­”æ¡ˆã€‚
    """
    with lock:
        # æµå¼è¾“å‡º
        chunks  = []
        print(f"\nğŸ’¡è®¤çŸ¥å·¥å…·=ç›´æ¥ä½œç­”")
        for chunk in llm.stream(prompt):
            print(chunk.content, end="", flush=True)
            chunks.append(chunk.content)

        response = "".join(chunks)

        # # å®Œæ•´è¾“å‡º
        # response = llm.invoke(prompt).content
        # print(f"\nğŸ’¡è®¤çŸ¥å·¥å…·=ç›´æ¥ä½œç­”\n{response}")

    return response


class RecognizingInput(BaseModel):
    query: str = Field(description="ä»»åŠ¡æŒ‡ä»¤")
